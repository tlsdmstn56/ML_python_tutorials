{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 내용은 [Keras Documentation](https://keras.io/) 를 참고하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model 시작하기\n",
    "***\n",
    "<code class=\"keras\">Sequential</code> 모델: 레이어의 선형 스택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성/ 파라미터 넘기기\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "# 직접 constructor에 파라미터를 넘김으로써 모델 생성 가능\n",
    "# Dense: input wire의 dimension을 설정\n",
    "# Acitivation: 레이어의 activation함수를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add 메소드를 통해 추가할 수도 있다. 위 코드와 strictly equivalent 하다고 한다.\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Shape 지정하기\n",
    "모델은 input shape을 알아야 한다. 따라서 모델의 첫 레이어(첫 번째만, 그 이후는 자동적으로 추론함)는 input shape에 대한 정보를 받아야 한다. 가능한 방법은 아래와 같다.\n",
    "\n",
    "1. 첫 레이어에 <code class=\"keras\">input_shape</code> 인수(argument)를 튜플 형태로 지정\n",
    "2. 2D 레이어에 대해(e.g. <code class=\"keras\">Dense</code>) <code class=\"keras\">input_dim</code> 인수를, 3D 입력 레이어에 대해 <code class=\"keras\">input_dim</code>과 <code class=\"keras\">input_length</code> 인수를 지정\n",
    "3. 고정된 배치 사이즈를 지정해야 하는 경우, <code class=\"keras\">batch_size</code> 인수를 레이어에 넘길 수 있다. 예를 들어, <code class=\"keras\">batch_size=32</code>와 <code class=\"keras\">input_shape=(6,8)</code> 인수를 레이어에 넘기는 경우 배치 shape은 <code class=\"keras\">(32,6,8)</code>이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1번 예시\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2번 예시\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 컴파일\n",
    "모델을 training하기 전에 compile 메소드를 통해 컴파일 해야 한다. compile 메소드는 3 가지 인수를 받는다.\n",
    "* optimizer: *optimizer*의 이름을 문자열로 넘기거나 <code class=\"keras\">Optimizer</code> 클래스의 인스턴스를 넘겨도 된다. [optimizer doc](https://keras.io/optimizers) 참고\n",
    "* loss function: 모델이 최소화하는 함수. 문자열을 넘기거나 objective function을 넘길 수 있다. [losses doc](https://keras.io/losses) 참고\n",
    "* a list of metrics: 어떤 종류의 분류 문제든 <code class=\"keras\">metrics=['accuracy']</code>을 사용할 수 있다. 이미 존재하는 metric의 문자열 또는 직접 metric함수를 만들어 넘길 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련/학습(Training)\n",
    "Numpy array로 입력 데이터와 레이블을 만들어 낸 후 <code class=\"keras\">fit</code> 메소드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cce8d2b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cce591908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras functional API 시작하기\n",
    "<code class=\"keras\">Sequential</code> 모델에 익숙하다고 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have shape (784,) but got array with shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f7770e563c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# starts training, data가 없어서 실행은 안됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1638\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_4 to have shape (784,) but got array with shape (100,)"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training, data가 없어서 실행은 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layer 인스턴스는 callable하며 tensor를 리턴한다.\n",
    "* Input tensor와 output tensor는 <code class=\"keras\">Model</code>을 정의하는데 쓰일 수 있다.\n",
    "* 이런 모델은 <code class=\"keras\">Sequential</code>과 거의 동일하게 동작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 모델은 callable하다 (레이어도 마찬가지)\n",
    "모델을 레이어처럼 취급 가능하다. 단, 모델을 호출한다는 것은 모델의 _구조_뿐 아니라 파라미터의 _가중치(weight)_도 그대로 이용한다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "# This works, and returns the 10-way softmax we defined above.\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같은 특성을 이용해 이미지 분류 모델을 비디오 분류 모델로 쉽게 바꾸는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Input tensor for sequences of 20 timesteps,\n",
    "# each containing a 784-dimensional vector\n",
    "input_sequences = Input(shape=(20, 784))\n",
    "\n",
    "# This applies our previous model to every timestep in the input sequences.\n",
    "# the output of the previous model was a 10-way softmax,\n",
    "# so the output of the layer below will be a sequence of 20 vectors of size 10.\n",
    "processed_sequences = TimeDistributed(model)(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 입력(Multi-input)과 다중 출력(Multi-output) 모델\n",
    "예를 들어 어떤 뉴스 헤드라인에 리트윗과 좋아요를 얼마나 받게 될지 예측하는 모델을 만든다고 한다. 주요 입력(main_input)은 단어들의 시퀀스, 즉 헤드라인 그 자체와  뉴스가 포스팅된 시간 등의 추가적인 입력(aux_input)이 있을 것이다. 이 모델은 2개의 loss function을 통해 supervised될 것이다. 주요 loss function을 모델 초기 단계에 사용하는 것은 좋은 정규화(regularization) 메커니즘이다.\n",
    "\n",
    "<img src=\"multi-input-multi-output-graph.png\" alt=\"multi-input-multi-output\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림대로 모델을 구현해보자. 위 그림에서 괄호 안은 클래스 이름을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구현:\n",
    "# main_input(InputLayer)\n",
    "# embedding_1(Embedding)\n",
    "# lstm_1(LSTM)\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# Note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구현:\n",
    "# aux_output(Dense)\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구현: 나머지\n",
    "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
    "x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main_input, auxiliary_input / main_output, auxiliary_output ==> 모델 정의\n",
    "model = Model(inputs=[main_input, auxiliary_input], \n",
    "                       outputs=[main_output, auxiliary_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "# 이름 지정하여 인수 넘기는 방법\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "              loss_weights={'main_output': 1., 'aux_output': 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headline_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f72124b8dcca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit([headline_data, additional_data], [labels, labels],\n\u001b[0m\u001b[1;32m      3\u001b[0m           epochs=50, batch_size=32)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 이름 지정하여 인수 넘기는 방법\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headline_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 Training\n",
    "model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "# 이름 지정하여 인수 넘기는 방법\n",
    "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "          {'main_output': labels, 'aux_output': labels},\n",
    "          epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Layers\n",
    "두 개의 트윗의 작성자가 같은지 알아보는 모델을 만든다고 하자. 이를 하기 위해서는 두 트윗을 두 개의 벡터로 만든 후 두 벡터를 붙여(concatenate) 로지스틱 회귀를 더한다. 이 모델의 결과물은 두 트윗의 작성자가 같을 확률이다. 모델은 positive tweet pairs과 negative tweet pairs 모두를 학습한다.\n",
    "\n",
    "문제가 대칭이므로 트윗의 처음부분(positive tweet pairs)을 encode하는 방법이 그대로 뒷 부분(negative tweet pairs)에 사용될 수 있다. 여기서는 LSTM 레이어를 사용한다.\n",
    "\n",
    "한 트윗에 대한 입력은 (280, 256)차원의 바이너리 행렬이다. 즉 280은 트위터 글자수 길이를 의미하며 256은 가장 많이 쓰이는 알파벳 256개의 포함 여부( [0/1] )이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_a = Input(shape=(280, 256))\n",
    "tweet_b = Input(shape=(280, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이러를 공유하기 위해서는 단순히 레이어 인스턴스를 한 번 만들고 각 입력 데이터에 대해 여러번 호출하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4b239cc0aa64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_a' is not defined"
     ]
    }
   ],
   "source": [
    "# This layer can take as input a matrix\n",
    "# and will return a vector of size 64\n",
    "# 인스턴스 생성은 한 번\n",
    "shared_lstm = LSTM(64)\n",
    "\n",
    "# When we reuse the same layer instance\n",
    "# multiple times, the weights of the layer\n",
    "# are also being reused\n",
    "# (it is effectively *the same* layer)\n",
    "encoded_a = shared_lstm(tweet_a)\n",
    "encoded_b = shared_lstm(tweet_b)\n",
    "\n",
    "# We can then concatenate the two vectors:\n",
    "merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis= -1)\n",
    "\n",
    "# And add a logistic regression on top\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "# We define a trainable model linking the\n",
    "# tweet inputs to the predictions\n",
    "model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([data_a, data_b], labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of layer \"node\"\n",
    "레이어 호출은 새로운 텐서(i.e. 레이어의 새로운 출력)를 생성하여 \"node\"를 그 레이어에 더하는 것을 의미한다. 다시 말해, 입력 텐서와 출력 텐서를 연결하는 것이다. 같은 레이어를 여러 번 호출하는 것은 그 레이어가 0부터 인덱싱되는 여러 노드를 소유하고 있다는 것을 의미한다.\n",
    "\n",
    "레이어가 하나의 입력만 연결되어있다면 혼동될 여지 없이 <code class=\"keras\">.output</code>은 레이어의 하나의 출력을 리턴할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Input(shape=(280, 256))\n",
    "\n",
    "lstm = LSTM(32)\n",
    "encoded_a = lstm(a)\n",
    "\n",
    "assert lstm.output == encoded_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer lstm_8 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-70b28c9e719b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mencoded_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 969\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer lstm_8 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "# ERROR: input이 여러 개면 indexing이 필요\n",
    "a = Input(shape=(280, 256))\n",
    "b = Input(shape=(280, 256))\n",
    "\n",
    "lstm = LSTM(32)\n",
    "encoded_a = lstm(a)\n",
    "encoded_b = lstm(b)\n",
    "\n",
    "lstm.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 인덱싱 \n",
    "assert lstm.get_output_at(0) == encoded_a\n",
    "assert lstm.get_output_at(1) == encoded_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code class=\"keras\">input_shape</code>과 <code class=\"keras\">output_shape</code> 속성도 <code class=\"keras\">output</code> 속성과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D\n",
    "a = Input(shape=(32, 32, 3))\n",
    "b = Input(shape=(64, 64, 3))\n",
    "\n",
    "conv = Conv2D(16, (3, 3), padding='same')\n",
    "conved_a = conv(a)\n",
    "\n",
    "# Only one input so far, the following will work:\n",
    "assert conv.input_shape == (None, 32, 32, 3)\n",
    "\n",
    "conved_b = conv(b)\n",
    "# now the `.input_shape` property wouldn't work, but this does:\n",
    "assert conv.get_input_shape_at(0) == (None, 32, 32, 3)\n",
    "assert conv.get_input_shape_at(1) == (None, 64, 64, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
