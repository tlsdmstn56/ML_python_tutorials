{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial settting\n",
    "from __future__ import print_function\n",
    "import os\n",
    "os.chdir(\"/usr/local/spark-2.3.0/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset vs Dataframe\n",
    "* Dataset: a distributed collection of data\n",
    "    * only in scala and java \n",
    "    * map, flatMap, filter ....\n",
    "* DataFrame: a _Dataset_ organized into named columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A simple example demonstrating basic Spark SQL features.\r\n",
      "Run with:\r\n",
      "  ./bin/spark-submit examples/src/main/python/sql/basic.py\r\n",
      "\"\"\"\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "# $example on:init_session$\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "# $example off:init_session$\r\n",
      "\r\n",
      "# $example on:schema_inferring$\r\n",
      "from pyspark.sql import Row\r\n",
      "# $example off:schema_inferring$\r\n",
      "\r\n",
      "# $example on:programmatic_schema$\r\n",
      "# Import data types\r\n",
      "from pyspark.sql.types import *\r\n",
      "# $example off:programmatic_schema$\r\n",
      "\r\n",
      "\r\n",
      "def basic_df_example(spark):\r\n",
      "    # $example on:create_df$\r\n",
      "    # spark is an existing SparkSession\r\n",
      "    df = spark.read.json(\"examples/src/main/resources/people.json\")\r\n",
      "    # Displays the content of the DataFrame to stdout\r\n",
      "    df.show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:create_df$\r\n",
      "\r\n",
      "    # $example on:untyped_ops$\r\n",
      "    # spark, df are from the previous example\r\n",
      "    # Print the schema in a tree format\r\n",
      "    df.printSchema()\r\n",
      "    # root\r\n",
      "    # |-- age: long (nullable = true)\r\n",
      "    # |-- name: string (nullable = true)\r\n",
      "\r\n",
      "    # Select only the \"name\" column\r\n",
      "    df.select(\"name\").show()\r\n",
      "    # +-------+\r\n",
      "    # |   name|\r\n",
      "    # +-------+\r\n",
      "    # |Michael|\r\n",
      "    # |   Andy|\r\n",
      "    # | Justin|\r\n",
      "    # +-------+\r\n",
      "\r\n",
      "    # Select everybody, but increment the age by 1\r\n",
      "    df.select(df['name'], df['age'] + 1).show()\r\n",
      "    # +-------+---------+\r\n",
      "    # |   name|(age + 1)|\r\n",
      "    # +-------+---------+\r\n",
      "    # |Michael|     null|\r\n",
      "    # |   Andy|       31|\r\n",
      "    # | Justin|       20|\r\n",
      "    # +-------+---------+\r\n",
      "\r\n",
      "    # Select people older than 21\r\n",
      "    df.filter(df['age'] > 21).show()\r\n",
      "    # +---+----+\r\n",
      "    # |age|name|\r\n",
      "    # +---+----+\r\n",
      "    # | 30|Andy|\r\n",
      "    # +---+----+\r\n",
      "\r\n",
      "    # Count people by age\r\n",
      "    df.groupBy(\"age\").count().show()\r\n",
      "    # +----+-----+\r\n",
      "    # | age|count|\r\n",
      "    # +----+-----+\r\n",
      "    # |  19|    1|\r\n",
      "    # |null|    1|\r\n",
      "    # |  30|    1|\r\n",
      "    # +----+-----+\r\n",
      "    # $example off:untyped_ops$\r\n",
      "\r\n",
      "    # $example on:run_sql$\r\n",
      "    # Register the DataFrame as a SQL temporary view\r\n",
      "    df.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    sqlDF = spark.sql(\"SELECT * FROM people\")\r\n",
      "    sqlDF.show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:run_sql$\r\n",
      "\r\n",
      "    # $example on:global_temp_view$\r\n",
      "    # Register the DataFrame as a global temporary view\r\n",
      "    df.createGlobalTempView(\"people\")\r\n",
      "\r\n",
      "    # Global temporary view is tied to a system preserved database `global_temp`\r\n",
      "    spark.sql(\"SELECT * FROM global_temp.people\").show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "\r\n",
      "    # Global temporary view is cross-session\r\n",
      "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:global_temp_view$\r\n",
      "\r\n",
      "\r\n",
      "def schema_inference_example(spark):\r\n",
      "    # $example on:schema_inferring$\r\n",
      "    sc = spark.sparkContext\r\n",
      "\r\n",
      "    # Load a text file and convert each line to a Row.\r\n",
      "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\r\n",
      "    parts = lines.map(lambda l: l.split(\",\"))\r\n",
      "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\r\n",
      "\r\n",
      "    # Infer the schema, and register the DataFrame as a table.\r\n",
      "    schemaPeople = spark.createDataFrame(people)\r\n",
      "    schemaPeople.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    # SQL can be run over DataFrames that have been registered as a table.\r\n",
      "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\r\n",
      "\r\n",
      "    # The results of SQL queries are Dataframe objects.\r\n",
      "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\r\n",
      "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\r\n",
      "    for name in teenNames:\r\n",
      "        print(name)\r\n",
      "    # Name: Justin\r\n",
      "    # $example off:schema_inferring$\r\n",
      "\r\n",
      "\r\n",
      "def programmatic_schema_example(spark):\r\n",
      "    # $example on:programmatic_schema$\r\n",
      "    sc = spark.sparkContext\r\n",
      "\r\n",
      "    # Load a text file and convert each line to a Row.\r\n",
      "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\r\n",
      "    parts = lines.map(lambda l: l.split(\",\"))\r\n",
      "    # Each line is converted to a tuple.\r\n",
      "    people = parts.map(lambda p: (p[0], p[1].strip()))\r\n",
      "\r\n",
      "    # The schema is encoded in a string.\r\n",
      "    schemaString = \"name age\"\r\n",
      "\r\n",
      "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\r\n",
      "    schema = StructType(fields)\r\n",
      "\r\n",
      "    # Apply the schema to the RDD.\r\n",
      "    schemaPeople = spark.createDataFrame(people, schema)\r\n",
      "\r\n",
      "    # Creates a temporary view using the DataFrame\r\n",
      "    schemaPeople.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    # SQL can be run over DataFrames that have been registered as a table.\r\n",
      "    results = spark.sql(\"SELECT name FROM people\")\r\n",
      "\r\n",
      "    results.show()\r\n",
      "    # +-------+\r\n",
      "    # |   name|\r\n",
      "    # +-------+\r\n",
      "    # |Michael|\r\n",
      "    # |   Andy|\r\n",
      "    # | Justin|\r\n",
      "    # +-------+\r\n",
      "    # $example off:programmatic_schema$\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    # $example on:init_session$\r\n",
      "    spark = SparkSession \\\r\n",
      "        .builder \\\r\n",
      "        .appName(\"Python Spark SQL basic example\") \\\r\n",
      "        .config(\"spark.some.config.option\", \"some-value\") \\\r\n",
      "        .getOrCreate()\r\n",
      "    # $example off:init_session$\r\n",
      "\r\n",
      "    basic_df_example(spark)\r\n",
      "    schema_inference_example(spark)\r\n",
      "    programmatic_schema_example(spark)\r\n",
      "\r\n",
      "    spark.stop()\r\n"
     ]
    }
   ],
   "source": [
    "%cat ./examples/src/main/python/sql/basic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launching spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"hello-world\") \\\n",
    "        .getOrCreate()\n",
    "       #.config(\"key\",\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untyped Dataset Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'],df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age']>21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queris Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('people')\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('people')\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# global temporary view can be used in another session \n",
    "# which are different from the session who make the global temporary view\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterOperating with RDDs\n",
    "RDD의 스키마 추론하는 방법\n",
    "* reflection: schema 오브젝트를 만들지 않고 넣기\n",
    "* schema 오브젝트를 만들어서 넣기\n",
    "#### reflection 이용, 스키마 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext # for RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\")) # 한 줄을 ,로 나누기\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # 첫 컬럼은 이름, 두 번쨰 컬럼은 나이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people은 RDD 형태이므로 이것을 DataFrame으로 변환\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame API\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Name: Justin']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using RDD API\n",
    "teenagers.rdd.map(lambda p: \"Name: \"+p.name).collect() # p는 dict 형태로 들어오는 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically하게 스키마 명시하기(직접 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type import\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the same lines, parts, and people\n",
    "# making schema\n",
    "schemaString = \"name age\"\n",
    "\n",
    "# list of StructField\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "# create StructType object using the list of StructField created above\n",
    "schema = StructType(fields)\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people, schema) # schema 직접 명시\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load/ Save\n",
    "[문서](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources) 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Usage Guide for Pandas with Apache Arrow\n",
    "### PyArrow install\n",
    "```bash\n",
    "# conda\n",
    "conda install -c conda-forge pyarrow\n",
    "\n",
    "#pip\n",
    "pip install pyarrow\n",
    "```\n",
    "### Enabling for vonersion to/from Pandas\n",
    "Spark DataFrame --> Pandas DataFrame by calling `toPandas()`\n",
    "\n",
    "Spark configuration `spark.sql.execution.arrow.enabled` must be `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf) # if arrow is not enabled, it raise error.\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324552</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.990966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.937478</td>\n",
       "      <td>0.256441</td>\n",
       "      <td>0.642096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088825</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.950739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.953102</td>\n",
       "      <td>0.468424</td>\n",
       "      <td>0.437852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.636838</td>\n",
       "      <td>0.429910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.142951</td>\n",
       "      <td>0.922369</td>\n",
       "      <td>0.761218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.831337</td>\n",
       "      <td>0.980546</td>\n",
       "      <td>0.465130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985369</td>\n",
       "      <td>0.404239</td>\n",
       "      <td>0.458919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.192842</td>\n",
       "      <td>0.372941</td>\n",
       "      <td>0.190815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.626144</td>\n",
       "      <td>0.584046</td>\n",
       "      <td>0.879541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.937827</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.785532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.266850</td>\n",
       "      <td>0.928404</td>\n",
       "      <td>0.433582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.646951</td>\n",
       "      <td>0.758424</td>\n",
       "      <td>0.667541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.668962</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.937771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.475457</td>\n",
       "      <td>0.597964</td>\n",
       "      <td>0.934421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.879516</td>\n",
       "      <td>0.785368</td>\n",
       "      <td>0.371657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.102484</td>\n",
       "      <td>0.622670</td>\n",
       "      <td>0.510380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.372183</td>\n",
       "      <td>0.042936</td>\n",
       "      <td>0.334615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.756654</td>\n",
       "      <td>0.596822</td>\n",
       "      <td>0.302350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.558369</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.492005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.559262</td>\n",
       "      <td>0.808296</td>\n",
       "      <td>0.403698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.530381</td>\n",
       "      <td>0.293708</td>\n",
       "      <td>0.074206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.643161</td>\n",
       "      <td>0.984638</td>\n",
       "      <td>0.309094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.113907</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.332837</td>\n",
       "      <td>0.257959</td>\n",
       "      <td>0.023019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.882226</td>\n",
       "      <td>0.718696</td>\n",
       "      <td>0.951180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.487499</td>\n",
       "      <td>0.615954</td>\n",
       "      <td>0.541839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.685296</td>\n",
       "      <td>0.248405</td>\n",
       "      <td>0.047878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.256725</td>\n",
       "      <td>0.390131</td>\n",
       "      <td>0.623582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.932251</td>\n",
       "      <td>0.478998</td>\n",
       "      <td>0.993764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.708252</td>\n",
       "      <td>0.384191</td>\n",
       "      <td>0.047522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.700093</td>\n",
       "      <td>0.085395</td>\n",
       "      <td>0.616112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.425009</td>\n",
       "      <td>0.821088</td>\n",
       "      <td>0.282223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.790282</td>\n",
       "      <td>0.397866</td>\n",
       "      <td>0.117302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.403283</td>\n",
       "      <td>0.870622</td>\n",
       "      <td>0.634461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.917250</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>0.464804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.081832</td>\n",
       "      <td>0.629895</td>\n",
       "      <td>0.291733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.364038</td>\n",
       "      <td>0.615275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.832114</td>\n",
       "      <td>0.119256</td>\n",
       "      <td>0.719874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.108226</td>\n",
       "      <td>0.445876</td>\n",
       "      <td>0.540474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.047156</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.585753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.890322</td>\n",
       "      <td>0.386601</td>\n",
       "      <td>0.483672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.983840</td>\n",
       "      <td>0.784656</td>\n",
       "      <td>0.622243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.424762</td>\n",
       "      <td>0.115622</td>\n",
       "      <td>0.855172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.784956</td>\n",
       "      <td>0.018534</td>\n",
       "      <td>0.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.809021</td>\n",
       "      <td>0.297036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.717535</td>\n",
       "      <td>0.261708</td>\n",
       "      <td>0.243484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.175628</td>\n",
       "      <td>0.655988</td>\n",
       "      <td>0.763370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.840766</td>\n",
       "      <td>0.267525</td>\n",
       "      <td>0.148303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.544150</td>\n",
       "      <td>0.899356</td>\n",
       "      <td>0.952784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.673860</td>\n",
       "      <td>0.275061</td>\n",
       "      <td>0.516467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.220057</td>\n",
       "      <td>0.314850</td>\n",
       "      <td>0.463216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.424821</td>\n",
       "      <td>0.843099</td>\n",
       "      <td>0.482776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.065690</td>\n",
       "      <td>0.952302</td>\n",
       "      <td>0.108222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.265529</td>\n",
       "      <td>0.315861</td>\n",
       "      <td>0.242475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.475438</td>\n",
       "      <td>0.132185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.438201</td>\n",
       "      <td>0.655549</td>\n",
       "      <td>0.617862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.188016</td>\n",
       "      <td>0.044266</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.340695</td>\n",
       "      <td>0.872192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.663307</td>\n",
       "      <td>0.293625</td>\n",
       "      <td>0.685693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.324552  0.804630  0.990966\n",
       "1   0.937478  0.256441  0.642096\n",
       "2   0.088825  0.364415  0.950739\n",
       "3   0.953102  0.468424  0.437852\n",
       "4   0.645455  0.636838  0.429910\n",
       "5   0.142951  0.922369  0.761218\n",
       "6   0.831337  0.980546  0.465130\n",
       "7   0.985369  0.404239  0.458919\n",
       "8   0.192842  0.372941  0.190815\n",
       "9   0.626144  0.584046  0.879541\n",
       "10  0.937827  0.055847  0.785532\n",
       "11  0.266850  0.928404  0.433582\n",
       "12  0.646951  0.758424  0.667541\n",
       "13  0.668962  0.573770  0.937771\n",
       "14  0.475457  0.597964  0.934421\n",
       "15  0.879516  0.785368  0.371657\n",
       "16  0.102484  0.622670  0.510380\n",
       "17  0.372183  0.042936  0.334615\n",
       "18  0.756654  0.596822  0.302350\n",
       "19  0.558369  0.109489  0.492005\n",
       "20  0.559262  0.808296  0.403698\n",
       "21  0.530381  0.293708  0.074206\n",
       "22  0.643161  0.984638  0.309094\n",
       "23  0.113907  0.019441  0.425700\n",
       "24  0.332837  0.257959  0.023019\n",
       "25  0.882226  0.718696  0.951180\n",
       "26  0.487499  0.615954  0.541839\n",
       "27  0.685296  0.248405  0.047878\n",
       "28  0.256725  0.390131  0.623582\n",
       "29  0.932251  0.478998  0.993764\n",
       "..       ...       ...       ...\n",
       "70  0.708252  0.384191  0.047522\n",
       "71  0.700093  0.085395  0.616112\n",
       "72  0.425009  0.821088  0.282223\n",
       "73  0.790282  0.397866  0.117302\n",
       "74  0.403283  0.870622  0.634461\n",
       "75  0.917250  0.055546  0.464804\n",
       "76  0.081832  0.629895  0.291733\n",
       "77  0.023166  0.364038  0.615275\n",
       "78  0.832114  0.119256  0.719874\n",
       "79  0.108226  0.445876  0.540474\n",
       "80  0.047156  0.748718  0.585753\n",
       "81  0.890322  0.386601  0.483672\n",
       "82  0.983840  0.784656  0.622243\n",
       "83  0.424762  0.115622  0.855172\n",
       "84  0.784956  0.018534  0.599400\n",
       "85  0.965755  0.809021  0.297036\n",
       "86  0.717535  0.261708  0.243484\n",
       "87  0.175628  0.655988  0.763370\n",
       "88  0.840766  0.267525  0.148303\n",
       "89  0.544150  0.899356  0.952784\n",
       "90  0.673860  0.275061  0.516467\n",
       "91  0.220057  0.314850  0.463216\n",
       "92  0.424821  0.843099  0.482776\n",
       "93  0.065690  0.952302  0.108222\n",
       "94  0.265529  0.315861  0.242475\n",
       "95  0.747938  0.475438  0.132185\n",
       "96  0.438201  0.655549  0.617862\n",
       "97  0.188016  0.044266  0.980769\n",
       "98  0.017316  0.340695  0.872192\n",
       "99  0.663307  0.293625  0.685693\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF( aka Vectorized UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas UDF는 Spark가 Arrow를 이용해서 데이터를 전송하여 Pandas가 데이터를 처리할 수 있도록 하는 user defined 함수이다. `pandas_udf`라는 데코레이터를 붙이거나 함수를 wrap함으로써 정의할 수 있다.\n",
    "\n",
    "현재는 2 가지의 Pandas UDF가 있다.\n",
    "* Scalar\n",
    "* Grouped Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar\n",
    "벡터화된 스칼라 연산에 사용된다. `select`나 `withColumn` 함수와 같이 사용될 수 있다. 내부적으로 Spark는 컬럼들을 배치로 나누고 각 배치마다 Pandas UDF를 호출한 후 다시 결합하여 Pandas UDF를 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# pandas에서 계산\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame에서 pandas udf를 호출하여 계산\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped Map\n",
    "`groupBy().apply()`와 같이 사용된다.\n",
    "\n",
    "* `DataFrame.groupBy`를 통해 데이터를 그룹을 나눈다.\n",
    "* 각 그룹마다 함수를 적용한다. 입력값과 출력값 모두 `pandas.DataFrame`이다.\n",
    "* 결과들을 합쳐 새로운 DataFrame을 만든다.\n",
    "\n",
    "`groupBy().apply()`를 사용하기 위해 다음 2개를 정의해야 한다.\n",
    "* 각 그룹별로 계산에 사용될 파이썬 함수\n",
    "* 출력할 `DataFrame`의 스키마가 정의된 `StructType` 객체나 스트링\n",
    "\n",
    "출력 스키마는 리턴하는 pandas.DataFrame의 컬럼의 이름이 아닌 위치에 따라 적용되므로 컬럼들이 반드시 인덱싱이 되어 있어야 한다.\n",
    "\n",
    "그룹 사이즈가 skewed되어 있으면 어떤 노드에서는 메모리가 부족할 수 있다. `maxRecordsPerBatch`는 그룹에는 적용되지 않으므로 유저가 알아서 그룹을 잘 분배해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").apply(substract_mean).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
