{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial settting\n",
    "from __future__ import print_function\n",
    "import os\n",
    "os.chdir(\"/usr/local/spark-2.3.0/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset vs Dataframe\n",
    "* Dataset: a distributed collection of data\n",
    "    * only in scala and java \n",
    "    * map, flatMap, filter ....\n",
    "* DataFrame: a _Dataset_ organized into named columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A simple example demonstrating basic Spark SQL features.\r\n",
      "Run with:\r\n",
      "  ./bin/spark-submit examples/src/main/python/sql/basic.py\r\n",
      "\"\"\"\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "# $example on:init_session$\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "# $example off:init_session$\r\n",
      "\r\n",
      "# $example on:schema_inferring$\r\n",
      "from pyspark.sql import Row\r\n",
      "# $example off:schema_inferring$\r\n",
      "\r\n",
      "# $example on:programmatic_schema$\r\n",
      "# Import data types\r\n",
      "from pyspark.sql.types import *\r\n",
      "# $example off:programmatic_schema$\r\n",
      "\r\n",
      "\r\n",
      "def basic_df_example(spark):\r\n",
      "    # $example on:create_df$\r\n",
      "    # spark is an existing SparkSession\r\n",
      "    df = spark.read.json(\"examples/src/main/resources/people.json\")\r\n",
      "    # Displays the content of the DataFrame to stdout\r\n",
      "    df.show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:create_df$\r\n",
      "\r\n",
      "    # $example on:untyped_ops$\r\n",
      "    # spark, df are from the previous example\r\n",
      "    # Print the schema in a tree format\r\n",
      "    df.printSchema()\r\n",
      "    # root\r\n",
      "    # |-- age: long (nullable = true)\r\n",
      "    # |-- name: string (nullable = true)\r\n",
      "\r\n",
      "    # Select only the \"name\" column\r\n",
      "    df.select(\"name\").show()\r\n",
      "    # +-------+\r\n",
      "    # |   name|\r\n",
      "    # +-------+\r\n",
      "    # |Michael|\r\n",
      "    # |   Andy|\r\n",
      "    # | Justin|\r\n",
      "    # +-------+\r\n",
      "\r\n",
      "    # Select everybody, but increment the age by 1\r\n",
      "    df.select(df['name'], df['age'] + 1).show()\r\n",
      "    # +-------+---------+\r\n",
      "    # |   name|(age + 1)|\r\n",
      "    # +-------+---------+\r\n",
      "    # |Michael|     null|\r\n",
      "    # |   Andy|       31|\r\n",
      "    # | Justin|       20|\r\n",
      "    # +-------+---------+\r\n",
      "\r\n",
      "    # Select people older than 21\r\n",
      "    df.filter(df['age'] > 21).show()\r\n",
      "    # +---+----+\r\n",
      "    # |age|name|\r\n",
      "    # +---+----+\r\n",
      "    # | 30|Andy|\r\n",
      "    # +---+----+\r\n",
      "\r\n",
      "    # Count people by age\r\n",
      "    df.groupBy(\"age\").count().show()\r\n",
      "    # +----+-----+\r\n",
      "    # | age|count|\r\n",
      "    # +----+-----+\r\n",
      "    # |  19|    1|\r\n",
      "    # |null|    1|\r\n",
      "    # |  30|    1|\r\n",
      "    # +----+-----+\r\n",
      "    # $example off:untyped_ops$\r\n",
      "\r\n",
      "    # $example on:run_sql$\r\n",
      "    # Register the DataFrame as a SQL temporary view\r\n",
      "    df.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    sqlDF = spark.sql(\"SELECT * FROM people\")\r\n",
      "    sqlDF.show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:run_sql$\r\n",
      "\r\n",
      "    # $example on:global_temp_view$\r\n",
      "    # Register the DataFrame as a global temporary view\r\n",
      "    df.createGlobalTempView(\"people\")\r\n",
      "\r\n",
      "    # Global temporary view is tied to a system preserved database `global_temp`\r\n",
      "    spark.sql(\"SELECT * FROM global_temp.people\").show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "\r\n",
      "    # Global temporary view is cross-session\r\n",
      "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\r\n",
      "    # +----+-------+\r\n",
      "    # | age|   name|\r\n",
      "    # +----+-------+\r\n",
      "    # |null|Michael|\r\n",
      "    # |  30|   Andy|\r\n",
      "    # |  19| Justin|\r\n",
      "    # +----+-------+\r\n",
      "    # $example off:global_temp_view$\r\n",
      "\r\n",
      "\r\n",
      "def schema_inference_example(spark):\r\n",
      "    # $example on:schema_inferring$\r\n",
      "    sc = spark.sparkContext\r\n",
      "\r\n",
      "    # Load a text file and convert each line to a Row.\r\n",
      "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\r\n",
      "    parts = lines.map(lambda l: l.split(\",\"))\r\n",
      "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\r\n",
      "\r\n",
      "    # Infer the schema, and register the DataFrame as a table.\r\n",
      "    schemaPeople = spark.createDataFrame(people)\r\n",
      "    schemaPeople.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    # SQL can be run over DataFrames that have been registered as a table.\r\n",
      "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\r\n",
      "\r\n",
      "    # The results of SQL queries are Dataframe objects.\r\n",
      "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\r\n",
      "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\r\n",
      "    for name in teenNames:\r\n",
      "        print(name)\r\n",
      "    # Name: Justin\r\n",
      "    # $example off:schema_inferring$\r\n",
      "\r\n",
      "\r\n",
      "def programmatic_schema_example(spark):\r\n",
      "    # $example on:programmatic_schema$\r\n",
      "    sc = spark.sparkContext\r\n",
      "\r\n",
      "    # Load a text file and convert each line to a Row.\r\n",
      "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\r\n",
      "    parts = lines.map(lambda l: l.split(\",\"))\r\n",
      "    # Each line is converted to a tuple.\r\n",
      "    people = parts.map(lambda p: (p[0], p[1].strip()))\r\n",
      "\r\n",
      "    # The schema is encoded in a string.\r\n",
      "    schemaString = \"name age\"\r\n",
      "\r\n",
      "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\r\n",
      "    schema = StructType(fields)\r\n",
      "\r\n",
      "    # Apply the schema to the RDD.\r\n",
      "    schemaPeople = spark.createDataFrame(people, schema)\r\n",
      "\r\n",
      "    # Creates a temporary view using the DataFrame\r\n",
      "    schemaPeople.createOrReplaceTempView(\"people\")\r\n",
      "\r\n",
      "    # SQL can be run over DataFrames that have been registered as a table.\r\n",
      "    results = spark.sql(\"SELECT name FROM people\")\r\n",
      "\r\n",
      "    results.show()\r\n",
      "    # +-------+\r\n",
      "    # |   name|\r\n",
      "    # +-------+\r\n",
      "    # |Michael|\r\n",
      "    # |   Andy|\r\n",
      "    # | Justin|\r\n",
      "    # +-------+\r\n",
      "    # $example off:programmatic_schema$\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    # $example on:init_session$\r\n",
      "    spark = SparkSession \\\r\n",
      "        .builder \\\r\n",
      "        .appName(\"Python Spark SQL basic example\") \\\r\n",
      "        .config(\"spark.some.config.option\", \"some-value\") \\\r\n",
      "        .getOrCreate()\r\n",
      "    # $example off:init_session$\r\n",
      "\r\n",
      "    basic_df_example(spark)\r\n",
      "    schema_inference_example(spark)\r\n",
      "    programmatic_schema_example(spark)\r\n",
      "\r\n",
      "    spark.stop()\r\n"
     ]
    }
   ],
   "source": [
    "%cat ./examples/src/main/python/sql/basic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launching spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"hello-world\") \\\n",
    "        .getOrCreate()\n",
    "       #.config(\"key\",\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untyped Dataset Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
